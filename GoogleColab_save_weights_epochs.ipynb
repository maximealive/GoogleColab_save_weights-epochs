{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleColab_save_weights_epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyTAeq+kCRrWWS4h2abw60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maximealive/GoogleColab_save_weights_epochs/blob/master/GoogleColab_save_weights_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjMHMwkrJKda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os, subprocess\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from keras.callbacks import *\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Mount Google Drive and create folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "work_dir = '/content/work_dir'                      # local work directory (faster)               \n",
        "dwld_dir = '/content/gdrive/My Drive/dwld_dir'      # drive download directory \n",
        "subprocess.call([\"mkdir\",\"-p\",work_dir])            # create directory for work\n",
        "subprocess.call([\"mkdir\",\"-p\",dwld_dir])            # create directory for dataset download\n",
        "\n",
        "os.chdir(dwld_dir)\n",
        "if not os.path.exists(dwld_dir + '/GoogleColab_save_weights_epochs'):\n",
        "  # Download from github repository\n",
        "  !git clone -l -s https://github.com/maximealive/GoogleColab_save_weights_epochs.git\n",
        "\n",
        "# Copy dataset(.zip) from 'dwld_dir/GoogleColab_save_weights_epochs' to 'work_dir' directory\n",
        "!cp /content/gdrive/My\\ Drive/dwld_dir/GoogleColab_save_weights_epochs/flower_photos.zip /content/work_dir/\n",
        "\n",
        "# Unzip dataset if not available\n",
        "os.chdir(work_dir)            \n",
        "if os.path.isdir('./flower_photos') == False: \n",
        "  !unzip -o -q flower_photos.zip -d .\n",
        "\n",
        "train_dir = pathlib.Path(work_dir + '/flower_photos/train/')\n",
        "test_dir = pathlib.Path(work_dir + '/flower_photos/test/')\n",
        "\n",
        "# Get class labels and number of classes\n",
        "CLASS_NAMES = np.array([item.name for item in train_dir.glob('*')])         # classes are subdirectories of '/ train /'\n",
        "NUM_CLASSES = len(CLASS_NAMES)                                              # total number of classes\n",
        "\n",
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "BATCH_SIZE = 8        # number of elements processed together in each iteration\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "IMG_CHANNELS = 3\n",
        "EPOCHS = 10           # number of training times\n",
        "\n",
        "# Create ImageDataGenerator by converting from uint8 to float32 in the range [0,1]\n",
        "train_data_gen = img_gen.flow_from_directory(directory=str(train_dir),\n",
        "                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                          color_mode='rgb',                                                     \n",
        "                                          classes = list(CLASS_NAMES),\n",
        "                                          class_mode='categorical',\n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_data_gen = img_gen.flow_from_directory(directory=str(test_dir),\n",
        "                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                          color_mode='rgb',                                                     \n",
        "                                          classes = list(CLASS_NAMES),\n",
        "                                          class_mode='categorical',\n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True)\n",
        "\n",
        "# Define a model\n",
        "def create_model():\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=6, \n",
        "                          kernel_size=(3, 3), \n",
        "                          strides = (1,1),\n",
        "                          padding = 'valid',\n",
        "                          activation='relu', \n",
        "                          input_shape=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)))\n",
        "  model.add(layers.AveragePooling2D())\n",
        "  model.add(layers.Conv2D(filters=16, \n",
        "                          kernel_size=(3, 3),\n",
        "                          strides = (1,1),\n",
        "                          padding = 'valid', \n",
        "                          activation='relu'))\n",
        "  model.add(layers.AveragePooling2D())\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(units=120, activation='relu'))\n",
        "  model.add(layers.Dense(units=84, activation='relu'))\n",
        "  model.add(layers.Dense(units=NUM_CLASSES, activation = 'softmax'))\n",
        "\n",
        "  LEARNING_RATE = 0.001 \n",
        "  model.compile(optimizer=optimizers.Adam(lr=LEARNING_RATE),            \n",
        "              loss='categorical_crossentropy',            \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "  \n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "\n",
        "checkpoint_path = '/content/gdrive/My Drive/_Projects/Git_Hub/weights/cp-{epoch:04d}.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# If the directory contains checkpoints they will be loaded\n",
        "if os.listdir(checkpoint_dir):\n",
        "  print('\\n###################################################\\n')\n",
        "  print('\\n             Restoring Weights & Epoch             \\n')\n",
        "  print('\\n###################################################\\n')\n",
        "\n",
        "  # Obtain latest checkpoint\n",
        "  latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  \n",
        "  # Number of epochs remaining\n",
        "  EPOCH_UPGRADED = int(latest[55:-5])\n",
        "  EPOCH_UPGRADED = EPOCHS - EPOCH_UPGRADED\n",
        "\n",
        "  # Load the previously saved weights and train the model\n",
        "  model.load_weights(latest)\n",
        "\n",
        "  # run on GPU\n",
        "  with tf.device('/GPU:0'):\n",
        "    history = model.fit_generator(\n",
        "        generator=train_data_gen,\n",
        "        steps_per_epoch=train_data_gen.n // BATCH_SIZE,      \n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_data_gen.n // BATCH_SIZE,\n",
        "        epochs=EPOCH_UPGRADED,\n",
        "        verbose=1,\n",
        "        callbacks=[cp_callback], \n",
        "        use_multiprocessing = True\n",
        "        )\n",
        "else:\n",
        "  # run on GPU\n",
        "  # Train the model\n",
        "  with tf.device('/GPU:0'):\n",
        "    history = model.fit_generator(\n",
        "        generator=train_data_gen,\n",
        "        steps_per_epoch=train_data_gen.n // BATCH_SIZE,      \n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_data_gen.n // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=1,\n",
        "        callbacks=[cp_callback], \n",
        "        use_multiprocessing = True\n",
        "        )\n",
        "\n",
        "# run on GPU   \n",
        "# Evaluate the model\n",
        "with tf.device('/GPU:0'):\n",
        "  test_loss, test_acc = model.evaluate_generator(generator=test_data_gen, \n",
        "                                                 steps = test_data_gen.n // BATCH_SIZE, \n",
        "                                                 verbose=1)\n",
        "\n",
        "print(f\"Model accuracy: {100*test_acc:.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}